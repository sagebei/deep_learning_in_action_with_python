{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgGyEP6Wn4lp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MemoryBuffer(object):\n",
    "    def __init__(self):\n",
    "        # 指定记忆库的大小\n",
    "        self.mem_size = 1000000\n",
    "        self.mem_cntr = 0\n",
    "        # 指定输入向量的长度\n",
    "        self.input_dims = 8\n",
    "        # 指定动作的个数\n",
    "        self.n_actions = 4\n",
    "        # 指定每次随机取样出的数据样本个数\n",
    "        self.batch_size = 64\n",
    "        # 定义用于存储状态、动作、奖励、下一个状态、回合结束的存储空间\n",
    "        self.state_memory = np.zeros((self.mem_size, self.input_dims))\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int8)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.next_state_memory = np.zeros((self.mem_size, self.input_dims))\n",
    "        self.done_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "    def store_memory(self, state, action, reward, next_state, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        # 对状态、动作、奖励、下一个状态、回合结束进行存储\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.done_memory[index] = 1 - int(done)\n",
    "        self.mem_cntr += 1\n",
    "    def sample(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        # 记忆库中随机取出批量的数据\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        dones = self.done_memory[batch]\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPrWgVxNoLki"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        # 指定输入向量的长度\n",
    "        self.input_dims = 8\n",
    "        # 指定动作空间\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        # 指定动作的个数\n",
    "        self.n_actions = 4\n",
    "        # 指定折扣率\n",
    "        self.gamma = 0.99\n",
    "        # 指定epsilon的值\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.996\n",
    "        self.epsilon_min = 0.01\n",
    "        # 指定批尺寸大小\n",
    "        self.batch_size = 64\n",
    "        self.memory = MemoryBuffer()\n",
    "        self.dqn = self.build_dqn()\n",
    "    # 构建DQN模型\n",
    "    def build_dqn(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=256, \n",
    "                        input_shape=(self.input_dims,), \n",
    "                        activation='relu'))\n",
    "        model.add(Dense(units=256, \n",
    "                        activation='relu'))\n",
    "        model.add(Dense(units=self.n_actions, \n",
    "                        activation=None))\n",
    "        model.compile(optimizer=Adam(lr=0.0005), \n",
    "                      loss='mse',\n",
    "                      metrics=None)\n",
    "        return model\n",
    "    # 存储状态、动作、奖励、下一个状态、回合结束信息\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.store_memory(state, action, reward, next_state, done)\n",
    "    # 根据当前状态使用贪婪策略选择一个动作\n",
    "    def choose_action(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        # 生成一个随机值\n",
    "        rand = np.random.random()\n",
    "        if rand < self.epsilon:\n",
    "            # 随机选择一个动作\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            # 使用DQN模型预测每一个动作对应的Q值\n",
    "            actions = self.dqn.predict(state)\n",
    "            # 采取对应Q值最大的动作\n",
    "            action = np.argmax(actions)\n",
    "        return action\n",
    "    # 训练DQN模型\n",
    "    def learn(self):\n",
    "        # 当存储空间中的数据个数大于批尺寸时才开始训练\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        # 取出批量的数据\n",
    "        state, action, reward, next_state, done = self.memory.sample(self.batch_size)\n",
    "        # 对当前状态每一个动作的Q值使用DQN模型进行预测\n",
    "        q_eval = self.dqn.predict(state)\n",
    "        # 对下一个状态每一个动作的Q值使用DQN模型进行预测\n",
    "        q_next = self.dqn.predict(next_state)\n",
    "        # 目标Q值\n",
    "        q_target = q_eval.copy()\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int8)\n",
    "        # 对目标Q值进行计算\n",
    "        q_target[batch_index, action] = reward + self.gamma * np.max(q_next, axis=1) * done\n",
    "        # 对DQN模型进行训练\n",
    "        self.dqn.fit(state, q_target, verbose=0)\n",
    "        # 在训练过程中逐渐减小epsilon的值\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "colab_type": "code",
    "id": "7MHtN0WFoNru",
    "outputId": "419cb390-f76d-47a1-9828-c897ce96f746"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "# 从模拟器中加载月球登陆游戏的环境\n",
    "env = gym.make('LunarLander-v2')\n",
    "# 使用智能体与环境进行50个回合的交互\n",
    "n_episodes = 500\n",
    "# 加载智能体\n",
    "agent = Agent()\n",
    "for i in range(n_episodes):\n",
    "    # 当前回合是够结束\n",
    "    done = False\n",
    "    # 当前回合获得的累计分数\n",
    "    total_reward = 0\n",
    "    # 获取到环境的初始状态\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        # 智能体根据状态选择一个行动\n",
    "        action = agent.choose_action(state)\n",
    "        # 环境根据智能体采取的行动作出反馈\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # 计算当前回合智能体获取到的累计奖励\n",
    "        total_reward += reward\n",
    "        # 将智能体与环境的交互信息进行存储\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        # 对智能体中的DQN模型进行训练\n",
    "        agent.learn()\n",
    "    print(f'Episode {i}/{n_episodes} ---> Total Reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mxs59x1koPzq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final DQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
