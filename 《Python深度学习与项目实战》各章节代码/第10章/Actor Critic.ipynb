{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.input_dims = 8\n",
    "        self.n_actions = 4\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.actor, self.critic, self.policy = self.build_actor_critic_network()\n",
    "    def build_actor_critic_network(self):\n",
    "        # 接收状态\n",
    "        inputs = Input(shape=(self.input_dims,))\n",
    "        # 接收优势值\n",
    "        advantages = Input(shape=[1])\n",
    "        dense1 = Dense(units=1024, \n",
    "                       activation='relu')(inputs)\n",
    "        dense2 = Dense(units=512, \n",
    "                       activation='relu')(dense1)\n",
    "        # 输出采取动作空间中每个动作的概率\n",
    "        outputs = Dense(units=self.n_actions, \n",
    "                        activation='softmax')(dense2)\n",
    "        # 状态值函数的输出\n",
    "        values = Dense(units=1, \n",
    "                       activation=None)(dense2)\n",
    "        # 自定义演员模型的损失函数\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            y_pred = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            log_lik = y_true * K.log(y_pred)\n",
    "            return K.sum(-log_lik * advantages)\n",
    "        # 构建演员模型\n",
    "        actor = Model([inputs, advantages], outputs)\n",
    "        actor.compile(optimizer=Adam(lr=0.00001), \n",
    "                      loss=custom_loss,\n",
    "                      metrics=None)\n",
    "        # 构建评判家模型\n",
    "        critic = Model(inputs, values)\n",
    "        critic.compile(optimizer=Adam(lr=0.00005), \n",
    "                       loss='mse',\n",
    "                       metrics=None)\n",
    "        # 构建策略模型\n",
    "        policy = Model(inputs, outputs)\n",
    "        return actor, critic, policy\n",
    "    def choose_action(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        # 预测当前状态下采取每一个行动的概率\n",
    "        probabilities = self.policy.predict(state)[0]\n",
    "        # 根据概率值选择一个行为\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        return action\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # 当前状态\n",
    "        state = state[np.newaxis, :]\n",
    "        # 下一个状态\n",
    "        next_state = next_state[np.newaxis, :]\n",
    "        # 当前状态值\n",
    "        critic_value = self.critic.predict(state) \n",
    "        # 下一个状态值\n",
    "        next_critic_value = self.critic.predict(next_state)\n",
    "        # 评判家模型的训练目标\n",
    "        target = reward + next_critic_value * (1 - int(done))\n",
    "        # 计算优势值\n",
    "        advantage = target - critic_value\n",
    "        # 将动作进行独热编码处理\n",
    "        action = to_categorical(action, num_classes=self.n_actions)\n",
    "        action = action[np.newaxis, :]\n",
    "        # 训练演员模型\n",
    "        self.actor.fit([state, advantage], action, verbose=0)\n",
    "        # 训练评判家模型\n",
    "        self.critic.fit(state, target, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "agent = Agent()\n",
    "env = gym.make('LunarLander-v2')\n",
    "n_episodes = 2000\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        agent.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "    print(f'Episode {i}/{n_episodes} ---> Total Reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
